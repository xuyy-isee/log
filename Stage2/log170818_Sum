2017.08.08(二)
    在forward_data函数中，主要功能是查找从当前节点到达目的节点的路径中需要经过的下一跳的节点号；
    由于任意两个节点之间都存在两条路由的路径，因此，可以在此处加入对比两条路由表项的下一跳链路哪个更优的函数，从而使得备用路由也派上用场；
    这里就要加入节点饱和度即节点队列中等待的包的数量，从而实现判断。

【问题】哪里可以找到每个节点队列中等待传送的包的数量？
send_hello和send_tc函数的最后都会根据当前是否是快速路有调用enque_msg函数，该函数应该是将包装好的hello和tc消息加到msg待发送的队列中，因此可以通过这个函数来查看队列中有多少等待发送的数据包。

已经做的修改：
1、将enque_msg函数修改为int类型，从而可以返回参数队列中等待数据包的数目，利用size()函数来统计等待数据包的数目——msgs_.size()，并作为参数返还给send_hello和send_tc函数；
2、在send_hello和send_tc函数的最后一部分加入了saturation的参数，用于存放由enque_msg返还的等待数据包数目，并根据数目的多少来判断当前节点的饱和程度，暂定5个包以下为空闲，5-20个包为一般，20个包以上为繁忙；
3、在OLSR_pkt.h的文档中，hello和tc消息结构中加入了节点饱和度的参数空间，定义为saturation；
4、在send_pkt函数中，将saturation加到hello和tc消息结构的最后面；
5、在recv函数中，将buffer中最后的saturation提取出来；
6、在forward_data函数中根据saturation的情况选择路径，这里需要修改文档OLSR_rtable.cc中的函数find_send_entry，需要找到两条路径再比较两条路径的优劣程度。
（其中6作为第二天早上的工作，并且将修改之后的代码进行运行，查看相关的功能是否奏效；最好在各个地方打印一下等待数据包的具体数目以及saturation的级别。）


2017.08.09(三)
在change14的代码中存在一定的问题：
1、在“ h = 3.”存在dist=2的路径，该路径和在“ h = 2.”中的dist=2的路径是相同的；
【解决方法】将函数rtable_computation()中的" h = 3."部分里面的for (u_int32_t h = 1; h<10; h++) 修改成为for (u_int32_t h = 2; h<10; h++) 即可；

2、修改之后又发现错误，以“main = 14”为例子，当节点14要跳转到节点16，则必然经过节点1；虽然在路由表中存在了两条路由表项，但是第二条路由通过节点2是无效的；因为虽然变成了3跳的路由，但是最终还是经过了节点1，这和2跳的路由实质上是一样的。
最理想的两条路由应该是
rtable1: main = 14, dest = 16, next = 1,  dist = 2, snr_min = 3.
rtable2: main = 14, dest = 16, next = 15,  dist = 3, snr_min = 3.
【问题】应该通过怎样的办法让代码优化呢？ 
————本来以为next = 15的那条路由是因为在next = 2的后面，而没有被替换所以才导致的；后来查看了topology，发现里面根本就没有存储16-0的链路信息，因此即使把所有路由表项都打印出来，都没有这条路由路径；
如果第二条路由是无用的，那就应该不被记录其中，只存储一条路由表项。


已经做的修改：
1、在forward_data函数中为根据saturation的情况选择路径，使用了新的lookupall函数，其存放位置为OLSR_rtable.cc文档中，并且比较找到的两个下一跳路径节点的饱和程度；
2、为比较节点的饱和程度，需要在多个地方存储节点的饱和程度，在定义中已经做的修改主要都在文档OLSR_repositories.h中，包括link_tuple、nb_tuple，主要是加了邻居节点的饱和度；由于在路由表中最主要关注的是下一跳节点的饱和度，因此只需要增加一跳邻居的饱和度即可；
3、



2017.08.10(四)
1、函数forward_data只在组网的时候才会使用，可见log20170810_2，代码存放在OLSR_change2_1中；
2、在2017.08.09中发现的第二个错误，现在可以解释原因了。之所以会在topology中没有找到16-0的链路，是因为节点只会选择它的mpr作为它在topology中的last_node，而节点16可以通过节点1到达它所有的二跳节点，因此它的mpr只有节点1，而topology中也只有与1相连的链路；


（1）log170718_4里面有拓扑打印。
（2）log170810_4中存在问题，如下所示，这里的next都是14，好像是可以两条路径：3-14-1-0/3-14-15-0，但是事实上0在topology中只可能存在一种last_node_id，也就是只能1或者只能15，因此上述的两种情况是不可能发生的。那么为什么会找到这样的两条路径呢？
rtable1: main = 3, dest = 0, next = 14,  dist = 3, snr_min = 3.
rtable2: main = 3, dest = 0, next = 14,  dist = 3, snr_min = 6.
（3）函数forward_data只在组网的时候才会使用，并且只会在dest=0的时候才使用，forward_data到底是哪里调用的？
（4）从log170810_6---8开始，函数forward_data在整个时间段内都有，不仅仅是组网，但是还是只会在dest=0的时候才使用；9就又回到原先的了；
（5）之所以在不组网的时候也存在，还想是因为在更新路由的时候也会有————————不一定，再仔细看。


2017.08.11(五)
1、昨天的log里面存在函数forward_data一直在打印的log中，应该就是老师之前所说的“快速路由一直无法结束”的问题；函数forward_data在现在的设置中应该是只存在于快速路由阶段，而从日志中的一句“[time] 24.790000 finish route node: 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 ”可以知道，后面全都没有更新，一直是这个状态，因此快速路由结束不了。
eg.在log170810_8中   rtable: main = 1, dest = 8  一直没有加入到路由表中，因此快速路有一直无法结束。

关于上面的问题应该是topology表只重新查找了两次，可能还存在节点的last在第二次在加入到路由表，而在前面的dest节点就不能在第二次利用之后才加入路由表的路由路径。
eg.在log170810_8中   rtable: main = 1, dest = 8  一直没有加入到路由表中，因此快速路有一直无法结束。

因此只要多遍历一下拓扑表就可以了，后再设置了5次，存放在OLSR_change2_2中。


2、之所以函数forward_data只在组网的时候才有，是因为后面没有发业务了，如果发业务的话也会出现；

3、之所以在组网的时候函数forward_data只在dest = 0的时候才出现，是因为节点0是簇头，设置所有的节点要向簇头发送单播，从而确定是否组网成功，后面的  [time] 24.790000 finish route node:  就是显示是否可以结束组网的。

4、路由转发的时候中间节点是不管这个数据包是否是自己产生的，更不在乎是谁发过来的，因此里面是没有源节点的信息的；如果要查看一个包的传送经过了哪些节点，可以在包经过某个节点的时候在包内做记录，从而记住经过的整个路径。不过这样不能把路径返回给源节点。

5、文档中的log170811_4是利用李东师姐选用路由的方法来跑的程序，主要是想查看一下“无法结束快速路由”这个现象；这里面的打印结果是正确的，能够正常结束路由。
从打印出来的情况来看，再结合代码分析，原先的方法是不会陷入快速路由不能出来的，它在创先路由的时候是通过h递增的方式一遍一遍遍历整个拓扑表的。
陷入快速路由是通过我的哪个方法才会出现的。

感觉我写的代码利用了好几层嵌套，如果网内的节点很多，那么在创建路由的时候可能会花费更多的时间，因此可以想办法优化。


周末计划：
1、
【问】
(1)“节点不相交多径”是怎么实现的，在网络中去除某些节点，是不是要等到下一个HELLO消息过来才可以？
(2)如果直接在本地链路中将其去掉是否可行？link_tuple/nb_tuple？
(3)是不是解决了两跳节点的两个路由路径不重合就解决了所有？
(4)topology_tuple到底是怎么建立的，是否只能是mpr节点？
(5)mpr节点的选取和计算是怎样的？(完成)

【答】
(2)参考mpr的计算方法，将已经被一跳节点覆盖的二跳节点从N2中删除，继续寻找下一个mpr。这里也可以建立一个类似与N1、N2的临时数组，存放link_tuple、nb_tuple、nb2hop_tuple、topology_tuple等，这样在寻找第二条路径的时候可以将已经使用过的节点删除。
不过也有一定的问题，因为主要解决的是第一条路由路径为2跳，备用路由路径为3跳，这种情况下路径不重合。而这种情况是分别在h=2和h=3中的，所以好像不能存放和删除节点。


2、rtable_computation函数中关于" h = 3."的部分代码可以精简和优化。



2017.08.12(六)
【答】
(5)mpr的计算发放就是按照log170813中的图片1\2来进行的。
但是统计了每个节点的MPR，发现节点14的mpr有1\3\15，其中节点15是不应该出现的，因此查找原因。
【问题】
修改了函数mpr_computation()中第1548行，出现了运行暂停的现象，照理来说应该没问题的……
	原先：for (nb2hopset_t::iterator it2 = it + 1; it2 != N2.end(); it2++) {
	修改：for (nb2hopset_t::iterator it2 = N2.begin(); it2 != N2.end(); it2++) {
【原因】
1、
这是因为原先的时候查找的是it之后的N2条目，如果删除也并不影响it的位置；
而现在查找的是整个N2条目，有可能会删除it之前的条目，因此，需要记录共删除了几条，并补偿回来。
2、
虽然补偿回来的，但是还是出现暂停的问题；
这是由于如果删除的是第一条，那么it--就到N2的最前面一条的还要前面了，这会出现错误；
3、
解决方法是记录现在所减的是否已经到N2的最前面了，如果没有，则继续减；如果到了，那之后就不用再减了；最多减的次数是之前记录的的delete_count。
在1576行加入修改，运行正常，运行的结果放在log170812_4中。
	if(N2.begin() == N2.end())
		break;

	for ( int ii =0;ii<delete_count; ii++ ){
		if( it == N2.begin() )
			break;
		else
			it--;
	}




2017.08.13（日）
【答】
(4)由于topology_tuple的更新的通过tc消息的，主要是通过其中的Advertised neighbor main address来更新的，接收到tc消息的节点从msg中提取出发送节点，若它是接收节点的邻居节点，那么就将其作为last_node_id，将自己作为dest_node_id，从而加入到topology_tuple中。
而tc消息中的Advertised neighbor main address地址是通过MPRSelect_tuple来加入的，因此topology_tuple中的条目都是mpr。



2017.08.14(一)
【答】
(1)
应该是要等到下一个HELLO消息来了才能更新的，不能在本地更新。
这主要是因为当前能够解决的最短节点不重合路由应该是二跳节点，想要使得主路由为2跳，备用路由为3跳，首先由于这个不是在同一个h当中的，所以不能进行标记，即使需要标记也不知道要预留多少空间；另外，由于" h=3."的时候所用到的是topology_tuple，这里面所选用的last_node_id是mpr节点，而mpr节点是根据TC消息来进行更新的，所以肯定要等到下一个TC过来才能更新mprset，之后再更新topology_tuple。
(3)
解决了最小单位二跳路由不重合并不能解决所有的。因为每个节点到达目的节点都有两种路径，中间在选择哪条的时候很有可能是和主路由是一样的，因为每次在选择的时候都是在两条路径中进行对比，选择出比较好的路径，所以有可能选择的是一样的。

选择备用路由是不存在节点饱和度的考虑的，因为备用路由只有在主路由失效的时候才会使用，因此当时已经没有发包了，也没有在队列中等待的数据包。



在" h=3."中使用的是topology来迭代找3跳及以上的路由，利用先找last_node_id的路由，再应用到dest_node_id的路由的方式进行；由于topology中dest和last一定是mprsel和mpr的关系，所以由于限定有些邻居节点是用不到的，这是一个局限。
那么利用同样的道理，如果是先找到本节点的邻居节点是否有到达目标节点的路由，从而来推算本节点到达目的节点的路由，那么就没有mpr的限制了。


截止目前的修改，将代码存放在OLSR_change2_3.cc中。


2017.08.14(二)
【总结】



2017.08.16(三)
之前一直在找Dijkstra算法的代码，但事实上工程中使用的就是这个算法。
与论文中介绍的不同的地方主要是每条链路的代价，在文献中介绍的几乎每条链路的代价都不相同，而在工程中将其设置为1，所以最后是用dist来代表每个目的节点到达源节点的代价。
与文献中不一样的是，由于每一条链路的代价不相同，所以即使跳数是相同的，也可以根据目的节点的代价来判断经过的唯一的一条路由路径，从而将中间节点的从网络中去除，之后在寻找备用路由，从而达到“节点不相交多径”的目的。
但是由于工程中没有代价这么一说，能够作为参考的只要dist，而源节点将数据包发给下一跳节点之后，相应的dist减一，那么从下一跳节点到达目的节点则只要满足跳数上的要求，就可以找到很多条路由路径，因此并不能通过这种方法来达到“节点不相交多径”的目的。



通过物理链路层对节点间链路接收到的平均信噪比进行检测，通过获取的平均信噪比在假设的链路信道模型下进行误码率以及误帧率计算以此作为链路质量的度量，再通过对一段连续时间内链路的质量进行连续采样作为历史数据对链路稳定性进行评估。同时，通过上层协议所提供的节点待发送数据量进行统计并根据历史数据统计出每个节点的饱和率。最后将所得的结果进行加群运算得出参考约束参数。


刚刚找到一个关于rtable_computation()的错误，如下：

    rtable1: main = 1, dest = 6, next = 14,  dist = 5, snr = 8.
    rtable2: main = 1, dest = 6, next = 14,  dist = 5, snr = 8.

即路由表中两条路由路径是相同的，这是由于在找的时候last_node_id不相同所导致的两条相同的路由，但事实上转发的时候并不知道路由表中所想要的last_node_id是什么，因此要避免该现象的发生。

改善之后的log存放在log170816_6中，前面的都是有问题的。
代码存放在OLSR_change2_4.cc中。




2017.08.17(四)
在分析代码中的process_tc()等函数以及打印日志之后，发现一些现象颠覆了之前一些考虑：
1、并不是每个节点都会发送tc消息的，只有部分节点会发送，且每次调试的时候发送tc的节点都不相同：log170817_1 : 1\3\5\6\7\11\12\14，log170817_2：0\1\2\3\4\5\6\9；
2、只有部分节点会发送tc消息，也就意味着拓扑表中并没有储存的所有节点的下一跳节点，last_node_id只有在1中被选为可以发送tc的节点才是，他们发送的tc通过广播到所有节点，并更新所有节点的拓扑表；
3、整个网络的拓扑表到最后确实是一模一样的，但是并不意味着整个网络只维护一张拓扑表，事实上每个节点都会维护一张属于自己的拓扑表，它的更新是通过msg.orig_node_id广播tc消息时某些节点接收到之后将orig节点的邻居节点加到自己的拓扑表中，虽然要使得所有节点都收到orig节点的广播需要很长的时间，但是肯定在组网结束之前能够让所有节点都完成拓扑表的更新，组网之后所有节点的拓扑结构都相同；
4、(1)HELLO消息是每个节点都会发，但是TC消息只有MPRSelector节点会发，MPR节点才会转发。
   (2)但是每个节点都会选择它们的MPR节点，即每个节点都是MPRSelector，但是并不是每个节点都是MPR节点。
   (3)综合1\2得出的结论：每个节点都是MPRSelector，则每个节点都会发送TC消息，与现象不符合。

	关于这一点的理解上是否会和现在发现的问题有关？

5、知道了！！！！
在工程中，只有MPR节点才会产生、发送和转发tc消息，它产生的tc消息中存放的Advertised Neighbor Main Address是选它为MPR的节点信息，即MPRSelector的信息；因此拓扑表中dest_node_id是MPRSelector节点，last_node_id是MPR节点；这也可以解释为什么拓扑表中的last_node_id只有一部分节点，而dest_node_id是所有节点都有的。


雷老师给了一篇论文让我消化，这是现在美军正在采用的优化路由的方法，主要是看看它所使用的方法是否能够用在我们的路由上，并有一定的优化作用。————《基于WNW战术波形网络的拓扑控制及路由优化研究》




2017.08.18(五)
在修改" h = 3."代码的时候，可以考虑从拓扑表的规律着手，拓扑表的排列顺序是按照last_node_id的，而last_node_id是mpr节点，mpr节点的个数并不多；可以事先遍历一遍拓扑表得到所有的mpr节点，然后遍历mpr节点表，将已经在路由表中的的mpr节点相连的dest_node_id加入到拓扑表中；再一次一次的遍历mpr表，直到所有的mpr节点都加入到路由表中，即所有的拓扑表中的目标节点都加入到路由表中，这样会减少index---topology---topology这样的嵌套模式，减少查表的次数和数量，从而达到优化这部分代码的目的。










